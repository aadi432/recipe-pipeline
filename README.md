# Recipe-Pipeline

# Recipe Analytics Pipeline (Firebase + Python)

This document serves as a complete **Data Engineering assignment submission**, covering data modeling, Firebase data setup, ETL/ELT workflows, data quality validation, analytics, and automated pipeline orchestration.

The project demonstrates how semi-structured NoSQL data from Firestore can be transformed into clean, validated, analytical datasets.

---

## ğŸ“Œ 1. Project Overview

This assignment implements an endâ€‘toâ€‘end data pipeline that:

* Inserts seed + synthetic recipe data into Firebase Firestore
* Exports Firestore collections into JSON
* Transforms JSON into normalized CSV tables
* Validates the CSV tables using custom dataâ€‘quality rules
* Generates analytical insights and visualization charts
* Runs all steps automatically through a single orchestrated pipeline script

Folder structure:

```
recipe-pipeline/
â”‚â”€â”€ analysis/          # Charts + insights
â”‚â”€â”€ exports/           # Raw JSON exports from Firestore
â”‚â”€â”€ outputs/           # Normalized CSVs + validation report
â”‚â”€â”€ scripts/           # ETL, validation, analytics, orchestration scripts
â”‚â”€â”€ seed_data.json     # Primary Pav Bhaji recipe
â”‚â”€â”€ serviceAccount.json
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ README.md
```

---

## ğŸ“Œ 2. Data Model (ER Diagram)

### **ER Diagram Overview**

```
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚     USERS        â”‚        â”‚     RECIPES      â”‚
   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚        â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
   â”‚ id (PK)          â”‚        â”‚ id (PK)          â”‚
   â”‚ name             â”‚        â”‚ title            â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚ description      â”‚
            â”‚                  â”‚ servings         â”‚
            â”‚                  â”‚ prep_time        â”‚
            â”‚                  â”‚ cook_time        â”‚
            â”‚                  â”‚ difficulty       â”‚
            â”‚                  â”‚ cuisine          â”‚
            â”‚                  â”‚ region           â”‚
            â”‚                  â”‚ created_at       â”‚
            â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                           â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  INTERACTIONS   â”‚      â”‚     INGREDIENTS      â”‚
   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚      â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
   â”‚ id (PK)         â”‚      â”‚ recipe_id (FK)       â”‚
   â”‚ recipe_id (FK)  â”‚      â”‚ ingredient_name      â”‚
   â”‚ user_id (FK)    â”‚      â”‚ ingredient_quantity  â”‚
   â”‚ type            â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â”‚ rating          â”‚
   â”‚ timestamp       â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚       STEPS       â”‚
   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
   â”‚ recipe_id (FK)    â”‚
   â”‚ step_order        â”‚
   â”‚ step_text         â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

This model ensures:

* 1:M relationship between **recipes â†’ ingredients**
* 1:M relationship between **recipes â†’ steps**
* M:N relationship between **users â†” recipes**, resolved through **interactions**

---

## ğŸ“Œ 3. Firebase Source Data Setup

The Firebase setup uses a **Firebase Service Account key** (`serviceAccount.json`) which acts as the secure authentication file required for connecting Python scripts to Firestore. This secret key must be placed inside the project root and **should never be shared publicly**.

### ğŸ” Service Account / Secret Key

* File required: **`serviceAccount.json`**
* Used for: authenticating all Firestore operations
* Must be stored securely and excluded from public repositories via `.gitignore`

### **Primary Recipe (Seed Data)**

The project uses **Pav Bhaji** from `seed_data.json` as the **primary recipe dataset**. This is the main real recipe provided by the candidate, as required by the assignment. It includes:

* Full list of ingredients
* Step-by-step cooking procedure
* Difficulty, time, and tags

### Additional Data Setup

Source data is created using `1_setup_firestore.py`.

### **Seed Recipe (Primary Dataset)**

* Pav Bhaji (from `seed_data.json`)
* This is the **main recipe provided by the candidate**, fulfilling the requirement to use your own recipe as the primary dataset
* Contains complete ingredients and step-by-step instructions
* Pav Bhaji (from `seed_data.json`)
* Contains complete ingredients and step-by-step instructions

### **Synthetic Recipes**

* 19 additional vegetarian recipes
* Random cuisine, region, difficulty, ingredients, steps

### **Users**

* 10 sample Indian users created

### **Interactions**

* 360 synthetic interactions including:

  * `view`
  * `like`
  * `cook_attempt`
  * random optional rating values

All data is inserted into three Firestore collections:

* `recipes`
* `users`
* `interactions`

---

## ğŸ“Œ 4. ETL / ELT Pipeline Steps

The ETL pipeline consists of five clearly separated stages.

### **Step 1 â€” Firestore Setup**

`1_setup_firestore.py` inserts all seed + synthetic data.

### **Step 2 â€” Export Firestore â†’ JSON**

`2_export_firestore.py` exports:

* `exports/recipes.json`
* `exports/users.json`
* `exports/interactions.json`

### **Step 3 â€” Transform JSON â†’ CSV**

`3_transform_to_csv.py` normalizes Firestore data into tables:

* `recipe.csv`
* `ingredients.csv`
* `steps.csv`
* `interactions.csv`
* `users.csv`

### **Step 4 â€” Data Validation**

`4_validate_csv.py` enforces:

* Required fields present
* Positive prep/cook times
* Sequential step order
* Valid difficulty values
* Rating validation

Output: `outputs/validation_report.json`

### **Step 5 â€” Analytics + Visualizations**

`5_analytics.py` generates:

* Ingredient frequency charts
* Difficulty distribution
* Correlation analysis
* Top views + likes
* User activity rankings
* Step count analysis

Outputs stored in `analysis/`.

---

## ğŸ“Œ 5. Data Quality Validation Rules

Validation ensures dataset consistency.

### **Recipe Validation**

* id, title, difficulty required
* prep/cook times must be â‰¥ 0
* difficulty âˆˆ {easy, medium, hard}
* tags must not be empty

### **Ingredient Validation**

* recipe_id, ingredient_name required

### **Interaction Validation**

* id, recipe_id, user_id, type required
* type must be valid
* rating must be numeric & within [1â€“5]

### **Steps Validation**

* step_order must be positive
* step sequence must be correct per recipe

---

## ğŸ“Œ 6. Analytics & Insights Generated

The analytics module produces at least 10 insights:

1. Most common ingredients
2. Average preparation time
3. Average total cooking time
4. Difficulty distribution
5. Prep-time vs likes correlation
6. Most viewed recipes
7. Ingredient frequency in high-engagement recipes
8. Step count distribution
9. Most active users
10. Full insights summary text file

Charts and summary files appear in the **analysis** folder.

---

## ğŸ“Œ 7. Pipeline Orchestration

To simulate a real data engineering workflow, all five stages are orchestrated using:

### **`run_pipeline.py`**

This script:

* Executes all pipeline steps in correct order
* Handles errors at script boundaries
* Automates data ingestion â†’ transformation â†’ validation â†’ analytics
* Ensures reproducibility and consistent results

Running one command completes the entire pipeline.

---

## ğŸ“Œ 8. How to Run the Pipeline

### **Install Dependencies**

```
pip install -r requirements.txt
```

### **Place Firebase Service Key**

Ensure `serviceAccount.json` exists in project root.

### **Run the Full Pipeline**

```
python scripts/run_pipeline.py
```

This will:

* Load & seed data into Firestore
* Export Firestore state into JSON
* Transform JSON into normalized CSVs
* Validate the CSVs
* Generate charts + insights

---

## ğŸ“Œ 9. Limitations

* Synthetic recipes are randomly generated (not real-world accurate)
* Ratings are partially random
* Pipeline assumes correct Firestore configuration
* Current dataset focuses only on vegetarian recipes

---

## ğŸ“Œ 10. Deliverables Included

* Complete ETL + validation scripts
* JSON exports
* Normalized CSV datasets
* Validation report
* Analytics charts
* Fully orchestrated pipeline runner
* This README documentation

---

## ğŸ“˜ Conclusion

This submission presents a structured, academic-style **end-to-end data engineering pipeline**. It demonstrates the transformation of NoSQL Firestore data into validated analytical datasets, supported by orchestration, modeling, validation, and visualization components.


