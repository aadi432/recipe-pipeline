ğŸ§‘â€ğŸ³ Recipe Analytics Pipeline (Firebase + Python)










A complete end-to-end Data Engineering Pipeline that ingests recipe data from Firebase Firestore, transforms it into analytical tables, validates the data using a custom Great-Expectations-style framework, and generates insights and visualizations â€” all automated through a Python orchestration script.

Designed as a production-ready Data Engineering assignment and a portfolio-quality project.

ğŸ“Œ Table of Contents

Project Overview

Folder Structure

Data Model (ER Diagrams)

Firestore Setup

ETL / ELT Pipeline

Data Quality Validation

Analytics & Visualizations

Pipeline Orchestration

How to Run

Limitations

Conclusion

ğŸš€ Project Overview

This project demonstrates a realistic Data Engineering workflow by transforming semi-structured Firestore data into clean, validated, and analytics-ready datasets.

âœ” Firestore ingestion (seed + synthetic data)
âœ” Export collections to JSON
âœ” Transform JSON â†’ CSV
âœ” Validate using custom expectations
âœ” Generate insights & charts
âœ” One-click pipeline automation

The primary dataset is a real recipe:
ğŸ¥˜ Pav Bhaji (from seed_data.json)
This fulfills the assignment requirement to include your own recipe.

ğŸ“ Folder Structure
recipe-pipeline/
â”‚â”€â”€ analysis/                # Charts + insight summaries
â”‚â”€â”€ exports/                 # Raw Firestore exports
â”‚â”€â”€ outputs/
â”‚     â”œâ”€â”€ clean/             # Normalized CSVs
â”‚     â”œâ”€â”€ validated/         # Validation reports
â”‚â”€â”€ scripts/                 # All ETL + validation + analytics scripts
â”‚â”€â”€ seed_data.json           # Pav Bhaji (primary dataset)
â”‚â”€â”€ serviceAccount.json      # Firestore auth key (ignored in git)
â”‚â”€â”€ .env                     # Contains FIREBASE paths
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ README.md

ğŸ—‚ï¸ Data Model (ER Diagrams)
ğŸ”· A. Firestore NoSQL Model (Nested)
 RECIPES
   â”œâ”€â”€ ingredients[]  
   â”œâ”€â”€ steps[]
   â”œâ”€â”€ metadata
   â””â”€â”€ tags[]

 INTERACTIONS
   â”œâ”€â”€ user_id â†’ USERS.id
   â”œâ”€â”€ recipe_id â†’ RECIPES.id
   â””â”€â”€ type (view/like/cook_attempt)

 USERS
   â””â”€â”€ id, name

ğŸ”· B. Normalized CSV ERD
RECIPES_CLEAN (PK: id)
INGREDIENTS_CLEAN (FK: recipe_id)
STEPS_CLEAN (FK: recipe_id)
INTERACTIONS_CLEAN (FK: recipe_id, user_id)
USERS_CLEAN (PK: id)

ğŸ”¥ Firestore Setup
âœ” Secure secret handling

serviceAccount.json stored in project root

.env stores secure paths

Both are ignored by .gitignore

âœ” Primary dataset

Pav Bhaji (your real recipe)

âœ” Synthetic data generated

19 vegetarian recipes

10 users

360 interactions (view/like/cook_attempt)

âœ” Retry logic added

All Firestore operations use automatic retry in case of temporary failures.

ğŸ”„ ETL / ELT Pipeline
Step 1 â†’ Ingest data into Firestore

scripts/1_setup_firestore.py

Inserts Pav Bhaji

Generates synthetic recipes

Creates users

Adds interactions

With retry logic + logging

Step 2 â†’ Export Firestore â†’ JSON

scripts/2_export_firestore.py
Exports:

recipes.json

users.json

interactions.json

Step 3 â†’ Transform JSON â†’ CSV

scripts/3_transform_to_csv.py
Outputs:

recipes_clean.csv

ingredients_clean.csv

steps_clean.csv

users_clean.csv

interactions_clean.csv

Step 4 â†’ Data Quality Validation
A. Basic Validation

scripts/4_validate_csv.py
Ensures:

Required fields

Valid difficulty

Step order

Positive times

B. Great-Expectations-style Validation

scripts/4a_custom_expectations_check.py
Generates:

outputs/validated/custom_ge_report.txt


Validates:

Column existence

Null checks

Unique IDs

Allowed values

Foreign key checks

Fully Python 3.14 compatible.

ğŸ“Š Analytics & Visualizations

scripts/5_analytics.py produces:

ğŸ“Œ Key charts

Top ingredients

Difficulty distribution

Prep time vs likes correlation

Top viewed recipes

Top active users

Step count distribution

Complexity score histogram

Engagement score rankings

ğŸ“Œ Summary insights

Saved as:

analysis/insights_summary.txt

ğŸ§© Pipeline Orchestration

Automated through:

scripts/run_pipeline.py


Runs all stages sequentially:

Firestore setup

Export JSON

Transform to CSV

Validate data

Generate analytics

One command controls the entire workflow.

ğŸ› ï¸ How to Run
1. Install dependencies
pip install -r requirements.txt

2. Add secrets

Place:

serviceAccount.json


in project root.

3. Configure .env
SERVICE_ACCOUNT_PATH=serviceAccount.json
PAV_SEED_PATH=seed_data.json

4. Run complete pipeline
python scripts/run_pipeline.py

âš ï¸ Limitations

Synthetic recipes are randomly generated

Ratings partially randomized

Assumes valid Firebase credentials

Dataset currently vegetarian-focused

ğŸ Conclusion

This project demonstrates a complete academic + production-style Data Engineering pipeline, covering:

âœ” NoSQL â†’ Structured ETL
âœ” Retry logic
âœ” Secure secret handling
âœ” Validation
âœ” Analytics
âœ” Full automation
âœ” ER modeling
âœ” Real + synthetic datasets
