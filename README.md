ğŸš€ Recipe Analytics Pipeline (Firebase + Python)

This repository contains a complete end-to-end Data Engineering pipeline that ingests recipe data from Firestore, transforms it into structured analytical tables, validates data quality using custom Great-Expectations-style rules, and generates insights through visualization and statistics.

This project simulates a real-world Data Engineering assignment with:

NoSQL â†’ Structured ETL

Retry logic

Validation

Analytics

Orchestration

Secure secrets

ERD diagrams

Clean folder structure

ğŸ“Œ 1. Project Overview

This pipeline performs the following:

âœ” Ingests seed + synthetic recipe data into Firestore

Including a primary real dataset: Pav Bhaji.

âœ” Exports Firestore collections to JSON
âœ” Transforms JSON to normalized CSV tables

Using professional flattening (ingredients & steps extracted properly).

âœ” Validates cleaned CSVs

Using:

Built-in validation

Custom GE-style validator (Python 3.14 compatible)

âœ” Generates analytics & charts
âœ” Runs end-to-end using one command

python scripts/run_pipeline.py

ğŸ“ 2. Folder Structure
recipe-pipeline/
â”‚â”€â”€ analysis/                # Charts + insight summaries
â”‚â”€â”€ exports/                 # Raw Firestore JSON exports
â”‚â”€â”€ outputs/
â”‚     â”œâ”€â”€ clean/             # Cleaned normalized CSVs
â”‚     â”œâ”€â”€ validated/         # Validation reports
â”‚â”€â”€ scripts/                 # ETL + validation + analytics + retry logic
â”‚â”€â”€ seed_data.json           # Primary Pav Bhaji seed recipe
â”‚â”€â”€ serviceAccount.json       # Firestore service key (ignored in git)
â”‚â”€â”€ .env                     # Secure variable storage
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ README.md

ğŸ“Œ 3. Data Model (ER Models)

This pipeline uses two data models:

ğŸ”· A. Firestore ERD (Source Model â€” Nested NoSQL)
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚       RECIPES       â”‚
 â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
 â”‚ id (PK)             â”‚
 â”‚ title               â”‚
 â”‚ description         â”‚
 â”‚ ingredients[]       â”‚â”€â”€â”€â”
 â”‚ steps[]             â”‚â”€â”€â”â”‚
 â”‚ difficulty          â”‚  â”‚â”‚ Nested arrays
 â”‚ prep_time           â”‚  â”‚â”‚
 â”‚ cook_time           â”‚  â”‚â”‚
 â”‚ tags[]              â”‚  â”‚â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚â”‚
              â”‚           â”‚â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”   â”‚â”‚
     â”‚  INTERACTIONS  â”‚   â”‚â”‚
     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚   â”‚â”‚
     â”‚ id (PK)        â”‚   â”‚â”‚
     â”‚ recipe_id (FK) â”‚â—€â”€â”€â”˜â”‚
     â”‚ user_id (FK)   â”‚â—€â”€â”€â”€â”€â”˜
     â”‚ type           â”‚
     â”‚ rating         â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚       USERS       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ”· B. CSV / Analytics ERD (Flattened Model)
 RECIPES_CLEAN
      â”‚ id (PK)
      â”‚ title, difficulty, prep_time, â€¦
      â”‚
      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â–¼               â–¼
INGREDIENTS_CLEAN   STEPS_CLEAN
recipe_id (FK)      recipe_id (FK)
ingredient_name     order
quantity            text

INTERACTIONS_CLEAN
recipe_id (FK)
user_id (FK)

USERS_CLEAN
id (PK)

ğŸ“Œ 4. Firebase Source Data Setup
ğŸ” Secrets (Stored Securely)

The project uses:

serviceAccount.json

.env â†’ contains SERVICE_ACCOUNT_PATH & PAV_SEED_PATH

Both ignored via .gitignore.

ğŸ“Œ Primary Dataset: Pav Bhaji (seed_data.json)

This is the main real recipe used to fulfill assignment requirements.

Includes:

Real ingredients

Real step-by-step instructions

Difficulty, region, cuisine metadata

ğŸ“Œ Synthetic Dataset Generated

The pipeline auto-generates:

19 vegetarian recipes

10 users

360+ interactions

Inserted into Firestore collections:

recipes

users

interactions

ğŸ“Œ 5. ETL / ELT Pipeline Steps
ğŸŸ¦ Step 1 â€” Setup Firestore

scripts/1_setup_firestore.py

Inserts Pav Bhaji (primary seed dataset)

Generates vegetarian recipes

Creates users

Inserts interactions

Includes retry logic for Firestore operations

Uses .env for secure secrets

ğŸŸ¦ Step 2 â€” Export Firestore â†’ JSON

scripts/2_export_firestore.py

Exports:

exports/recipes.json
exports/users.json
exports/interactions.json


Includes:

Retry logic

Logging

ğŸŸ¦ Step 3 â€” Transform JSON â†’ Normalized CSV

scripts/3_transform_to_csv.py

Outputs (clean tables):

outputs/clean/recipes_clean.csv
outputs/clean/ingredients_clean.csv
outputs/clean/steps_clean.csv
outputs/clean/users_clean.csv
outputs/clean/interactions_clean.csv

ğŸŸ¦ Step 4 â€” Data Validation

Two layers:

A. Basic Validation (4_validate_csv.py)

Checks:

Required columns

No missing recipe/user IDs

Positive times

Step ordering

Duplicate detection

B. Custom Great-Expectations-Style Validation

scripts/4a_custom_expectations_check.py

Generates:

outputs/validated/custom_ge_report.txt


Checks:

Column existence

Null checks

Uniqueness

Integer-like values

Foreign key integrity

100% Python 3.14 compatible.

ğŸŸ¦ Step 5 â€” Analytics & Visualization

scripts/5_analytics.py

Generates:

Ingredient frequency chart

Difficulty distribution

Top 10 viewed recipes

Step count distribution

User activity

Prep vs likes correlation

Complexity score

Engagement score

Outputs (examples):

analysis/top_ingredients.png
analysis/difficulty_distribution.png
analysis/top_viewed_recipes.png
analysis/complexity_distribution.png
analysis/insights_summary.txt

ğŸ“Œ 6. Key Data Quality Rules
âœ” Recipes

Must contain id, title, difficulty

prep/cook times â‰¥ 0

difficulty âˆˆ {easy, medium, hard}

âœ” Ingredients

recipe_id required

ingredient_name required

âœ” Steps

valid integer step order

all steps must belong to a recipe

âœ” Interactions

recipe_id and user_id must exist

type âˆˆ {view, like, cook_attempt}

rating âˆˆ {1â€“5 or NULL}

ğŸ“Œ 7. Orchestration (Automated Pipeline)

run_pipeline.py orchestrates:

1. Setup Firestore
2. Export JSON
3. Transform CSV
4. Validate data
5. Run analytics


One command runs the entire pipeline:

python scripts/run_pipeline.py

ğŸ“Œ 8. How to Run
1ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

2ï¸âƒ£ Place Firestore Key

Place your secure:

serviceAccount.json


inside project root.

3ï¸âƒ£ Set environment variables

Create .env:

SERVICE_ACCOUNT_PATH=serviceAccount.json
PAV_SEED_PATH=seed_data.json

4ï¸âƒ£ Run Pipeline
python scripts/run_pipeline.py

ğŸ“Œ 9. Limitations

Synthetic recipes not real-world accurate

Ratings are randomly generated

Project assumes working Firestore configuration

Primarily vegetarian dataset

ğŸ“Œ 10. Deliverables Included

Full ETL pipeline

JSON exports

Normalized CSV datasets

Custom data quality validation (GE-style)

Charts + insights

Complete orchestration

Full README for assignment

ğŸ“˜ Conclusion

This project demonstrates a fully functional Data Engineering workflow, transforming semi-structured Firestore data into validated analytical datasets. It includes:

Data modeling

Secure key handling

Firestore ingestion

ETL/ELT processing

Data validation

Analytics

Orchestration

It is structured, professional, and suitable for academic submission or real-world Data Engineering evaluation.
